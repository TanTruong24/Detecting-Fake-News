{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCkuuat8k5EY"
      },
      "source": [
        "<center> <h1> <b> CS114.L21: MACHINE LEARNING <center> <h1> <b>\n",
        "\n",
        "<center> <h2> <b> BÀI TẬP COLAB SỐ 04: WEB SCRAPING (Ngày 07/6/2021) <center> <h> <b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcnKBJO4k-S4"
      },
      "source": [
        "###**1. Nhóm thực hiện**\n",
        "\n",
        "* 19522246 - Vũ Nguyễn Nhật Thanh\n",
        "* 19522180 - Trương Thế Tấn\n",
        "* 19521551 - Nông Thanh Hồng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3oxAfxSi66H"
      },
      "source": [
        "#**I. Đánh giá, nhận xét chung**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca18NzAuluIv"
      },
      "source": [
        "**Nhận xét 1:**\n",
        "\n",
        "Việc crawl data từ các *trang tin châm biếm dễ dàng hơn những trang tin chính thống*. Lý do mà nhóm em nhận thấy là:\n",
        "\n",
        "  - Trang châm biếm *(The Beaverton, The Burrard Street Journal, Cracked)* có cấu trúc đơn giản, thường là **trang web tĩnh** *(static website)* sử dụng HTML chủ yếu nên có thể sử dụng BeautifulSoup một cách khá đơn giản để lấy tiêu đề và url. Một chút phức tạp hơn là có sự chuyển tiếp giữa các trang *(next page)*, tuy nhiên vẫn khá đơn giản vì chỉ cần thay đổi chỉ số trang.\n",
        "    - Ví dụ:\n",
        "      - https://thebeaverton.com/page/1\n",
        "      - https://thebeaverton.com/page/2\n",
        "\n",
        "  - Trong Khi đó những trang báo chính thống nổi tiếng *(The New York Times, The Sun Times, The Washington Post)* sử dụng cấu trúc phức tạp hơn, là các **trang web động** *(dynamic website)* trong đó có việc kết hợp Javascript và HTML *(có thể có thêm những cái khác)*. Điển hình là trang [The Washington Post](https://www.washingtonpost.com/) có nút ***Load More*** để tải thêm các bài báo, hoặc là trang [The New York Times](https://www.nytimes.com/) sử dụng ***cuộn chuột*** để tải thêm bài báo.\n",
        "\n",
        "       Đồng thời, các trang chính thống chia ra nhiều mục, chủ đề khác nhau, mỗi chủ đề lại chia ra các chủ đề nhỏ hơn nên gây khó khăn trong việc crawl.\n",
        "\n",
        "\n",
        "**Nhận xét 2:**\n",
        "\n",
        " - Phải kết hợp nhiều phương pháp khác nhau để crawl data từ các trang chính thống. Điển hình như trong code của nhóm em sử dụng **Selenium** kết hợp với BeautifulSoup và Scrapy. \n",
        "\n",
        " - Đối với các trang chính thống, dù được chia thành nhiều chủ đề khác nhau nhưng vẫn có thể lấy được nhiều bài báo bằng các mục như ***Today’s Paper** của [The New York Times](https://www.nytimes.com/) hoặc tìm theo các **Topic**.\n",
        "\n",
        "**Nhận xét 3:**\n",
        "\n",
        "Một số trang web sẽ chặn crawl data do việc bạn sử dụng urllib dựa trên tác nhân người dùng *(lỗi 403)* hoặc ngắt kết nối. Ví dụ như trang [The Washington Post](https://www.washingtonpost.com/). Nhóm khác phục bằng cách thêm dòng lệnh vào file **setting.py** của Scrapy dòng lệnh sau:\n",
        "\n",
        "```\n",
        "USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"\n",
        "```\n",
        "\n",
        "**Nhận xét 4:**\n",
        "\n",
        "Các code từ BeautifulSoup và Selenium được nhóm viết trên Colab.\n",
        "\n",
        "  - Ưu điểm của việc này là dễ chia sẻ mã. Tiết kiệm được không gian bộ nhớ, không cần cài đặt cái package trên máy.\n",
        "  - Nhược điểm của việc này đó là có những package không có sẵn như **selenium** nên mỗi lần chạy sẽ phải chạy lại cài đặt. Ví dụ như việc sử dụng Scrapy trên Colab khá khó khăn vì có project sẵn máy. Ngoài ra việc sử dụng Webdriver trên Colab cũng bất tiện, mỗi lần chạy mới đều phải chạy code:\n",
        "\n",
        "  ```\n",
        "# install chromium, its driver, and selenium\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "# set options to be headless, ..\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "  ```\n",
        "\n",
        "**Nhận xét 5:**\n",
        "\n",
        "- Sẽ đỡ mất thời gian nếu nhóm viết code để khi crawl dữ liệu về thành file json luôn. Do không chú ý kĩ nên nhóm tạo file csv để lưu dữ liệu crawl về. Điều đó mất thêm thời gian để nối các file csv và chuyển sang file json.\n",
        "\n",
        "**Nhận xét 6:**\n",
        "\n",
        "- Có sự lệch data giữ tin châm biếm và tin chính thống. Trong data của nhóm thì tin chính thống chiếm 2/3 và tin châm biếm chiếm 1/3.\n",
        "\n",
        "-  Đối với các trang chính thống nhóm crawl từ năm 2018 đến 2021 (tháng 5), nhưng đối với trang tin châm biếm thì crawl hết các bài báo có thể có từ năm thành lập, bởi số lượng khá ít.\n",
        "\n",
        "**Đánh giá**\n",
        "\n",
        "- Nhóm đã crawl được tổng là: **337758 dòng**\n",
        "\n",
        "- Nhóm làm chưa thực sự tốt, chưa phân bố thời gian và nhân lực hợp lý cho từng trang. Việc quá sa vào fix lỗi cho trang The Washington Post đã làm mất thời gian của nhóm *(thay thế bằng trang Newsweek.com)*. Không chú ý kĩ yêu cầu nộp bài của thầy dẫn đên mắc thời gian cho việc chỉnh sửa lại file csv sang json. \n",
        "- Đánh giá việc thời gian chạy chương trình chủ quan, nghĩ một trang chạy tối đa trong 2 giờ nhưng thực tế có trang đã lên 5 giờ.\n",
        "- Chưa có sự giao lưu, hợp tác với các nhóm khác.\n",
        "\n",
        "- Rút kinh nghiệm cho những lần sau làm tốt hơn.\n",
        "\n",
        "- Đánh giá cuối nhóm làm chưa tốt bài tập được giao\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02cz7msls3Nm"
      },
      "source": [
        "#**II. Báo cáo về quá trình crawl dữ liệu của từng trang**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWVbGl85tEd7"
      },
      "source": [
        "##**1. The Beaverton**\n",
        "\n",
        "- link: https://thebeaverton.com/\n",
        "\n",
        "- Là  một trang Châm biếm, ở *The Beaverton* có **778 trang** *(ngày 10/6/2021)* và được thành lập năm 2003. Tuy nhiên những năm đầu thành lập *(2003, 2004, ...)* trang chỉ được 1, 2 bài mỗi năm nên nhóm quyết định lấy từ 1 đến 500 trang được crawl dữ liệu. Thực tế sau khi chạy thì trang bài báo cuối cùng được crawl là viết năm 2016 *(cách 4 năm)* và nhóm lấy kết quả này.\n",
        "\n",
        "- Một vấn đề khác ở việc lấy ngày xuất bản của mỗi bài báo. Do nhóm chỉ lấy được ngày, giờ xuất bản của những bài báo chính (10 bài hiển trị chính trên mỗi trang) còn những *bài báo đi kèm theo, đề xuất* thì nhóm không trích xuất được nên mới dẫn đến việc khác nhau số phần tử **list pulishdate** và **title, aricle_link**. Mỗi trang crawl được 121 bài và 10 ngày xuất bản, nên những giá trị trống của ngày xuất bản được thay bằng NaN (111 giá trị mỗi trang)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xrxa34Kt3q2"
      },
      "source": [
        "##**2. The Burrard Street Journal**\n",
        "\n",
        "- Là một trang Châm biếm **The Burrad Street Journal** thành lập năm 2016 nên số lượng bài còn khá ít (526 bài) nên nhóm crawl hết bài báo hiện có.\n",
        "\n",
        "- Trang hiển thị các bài báo theo thời gian xuất bản. Khi muốn xem nhiều tin cũ hơn phải ấn sang page tiếp theo, và trang được cập nhật như sau:\n",
        "\n",
        "  *https://www.burrardstreetjournal.com/page/1*\n",
        "\n",
        " Bằng cách tăng dần các số (2,3,4,...) biểu thị cho số trang từ 1 được 47 trang (web hiện có 46 trang - ngày 12/6/2021).\n",
        "\n",
        "- Tiêu đề bài báo và link bài báo lần lượt nằm trong ***title*** và ***href*** với ***class = \"entry-title td-module-title\"***. Còn ngày xuất bản ***publishdate*** được tìm thấy ở ***class = \"td-post-date\"***.\n",
        "\n",
        "- Trong các bài báo **đều có ngày xuất bản kèm theo bên dưới**, tuy nhiên ở trang 1 *(trang chính của website)* có một số tiêu đề bài báo bị trùng. Minh chứng là số tiêu đề crawl được từ trang đầu của Website có 50 tiêu đề,  nhưng chỉ thu được 41 ngày xuất bản, dẫn đến dư thừa 9 tiêu đề. Vấn đề này chỉ xảy ra ở trang đầu *(trang chính của website)*. Với những trang còn lại *(từ trang 2 đến 47)* thì mỗi trang chỉ chứa 14 bài báo có đầy đủ ngày tháng năm. Nên việc bị dư thừa 9 bài báo, cộng thêm việc lấy ngày tháng năm xuất bản không thực sự cần thiết trong dữ liệu nên với những giá trị bị thiếu của cột **pulishdate** nhóm sẽ thêm *NaN* vào thay thế. Điều này lại dẫn đến việc là 50 hàng đầu tiên trong file **1-TheBurrardStreetJournal.csv** sẽ có thời gian xuất bản không chính xác hoàn toàn so với thực tế, các bài báo trong trang chính xuất bản trong vòng 1 tuần đổ lại và chỉ có 9 dữ liệu nên nhóm quyết định bỏ qua việc xử lý các 9 dữ liệu này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyX12i0LxHzO"
      },
      "source": [
        "##**3. The New York Times**\n",
        "\n",
        "- The New York Times (**NYTimes**) là một trang web động. Cũng giống các trang chính thống khác thì **NYTimes** có rất nhiều chủ đề ví dụ như: *Politics\n",
        "N.Y., Business, Opinion, Tech, Science, Health, Sports, Arts, Books,...*\n",
        "\n",
        "- Ở **NYTimes** có mục **Today’s Paper** với một ví dụ url dưới đây: \n",
        "\n",
        "  https://www.nytimes.com/issue/todayspaper/2021/06/03/todays-new-york-times\n",
        "\n",
        "  Dễ nhận ra url ở trên có dạng:\n",
        "\n",
        "  https://www.nytimes.com/issue/todayspaper/< year >/< moth >/< day >/todays-new-york-times\n",
        "\n",
        "  Với ***month:*** *01, 02, 03, 04,...,11, 12*. Và ***day:*** *01, 02, 03,...*\n",
        "\n",
        "- Nhóm thực hiện 3 vòng lặp for lần lượt:\n",
        "```\n",
        "base_url = \"https://www.nytimes.com/issue/todayspaper/\"\n",
        "for y in year:\n",
        "    if (y == \"2021\"):\n",
        "        month.clear()\n",
        "        month = [\"05\", \"04\", \"03\", \"02\",\"01\"]\n",
        "    for m in month:\n",
        "        for d in date:\n",
        "          urls = base_url + y + \"/\" + m + \"/\" + d + \"/todays-new-york-times\"\n",
        "```\n",
        "Nhóm xét năm 2018, 2019, 2020, 2021 với năm 2021 chỉ lấy 5 tháng đầu tiên.\n",
        "\n",
        "- Ở mỗi trang **NYTimes** sẽ không tải hết bài báo và phải cuộc xuống để tải thêm. Sau nhiều lần thử nghiệm thì rút ra, cuộn 3 lần sẽ ra hết bài báo và vị trí khi cuộc tới bài báo sẽ tải thêm đó là khoảng 9/10 độ dài trang. Nhóm đã sử dụng Selenium trong việc thực hiện cuộn này:\n",
        "  ```\n",
        "  scroll_pause_time = 5\n",
        "            i = 0\n",
        "            while i < 3:\n",
        "                if (i != 0):\n",
        "                    browser.execute_script(\"window.scrollTo(0, 0);\")\n",
        "                    time.sleep(2)\n",
        "                    browser.execute_script(\n",
        "                        \"window.scrollTo(0, ((document.body.scrollHeight*9)/10));\")\n",
        "                    # Wait to load page\n",
        "                    time.sleep(scroll_pause_time)\n",
        "                else:\n",
        "                    browser.execute_script(\n",
        "                        \"window.scrollTo(0, (document.body.scrollHeight*3)/4);\")\n",
        "                i += 1\n",
        "    ```\n",
        "  Trong quá trình cuộn phải có thời gian nghỉ để trang kịp tải. Ở lần đầu cuộc, nhóm cuộc tới vị trí bằng 3/4 độ dài trang, sau đó lại đưa thanh về vị trí trên cùng rồi tiếp tục cuộn tới vị trí bằng 9/10 trang.\n",
        "\n",
        "- Một vấn đề nhỏ khác url bài báo khi trích xuất được ở dạng:\n",
        "\n",
        "  */2018/11/30/world/asia/john-chau-andaman-missionary.html*\n",
        "\n",
        "  nên cần phải thêm đoạn https://www.nytimes.com/ để thành một đường link hoàn chỉnh.\n",
        "\n",
        "  https://www.nytimes.com/2018/11/30/world/asia/john-chau-andaman-missionary.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sDSktlJ5W78"
      },
      "source": [
        "##**2. The Sunday Times**\n",
        "\n",
        "- Nhóm tìm được mục **topic** gồm nhiều chủ đề khác nhau. Mỗi chủ đề sẽ có nhiều trang với các bài báo được xếp theo thời gian phát hành.\n",
        "- Một ví dụ về đường link: \n",
        "  \n",
        "  https://www.thetimes.co.uk/topic/uk-politics?page=2 \n",
        "\n",
        "  Đường link có thể phân tích phần \n",
        "  \n",
        "  https://www.thetimes.co.uk/topic + topic + ?page= + num_page.\n",
        "\n",
        "- Nhóm chọn 14 topic phổ biến nhất là \n",
        "```\n",
        "#14 topic\n",
        "topic = [ \"uk-politics\", \"global-politics\", \n",
        "          \"health\", \"technology\", \"transport\", \"science\", \"law\", \n",
        "          \"economics\", \"markets\", \"personal-finance\", \"real-estate\", \"banking\", \n",
        "          \"tennis\", \"golf\" ]\n",
        "```\n",
        "\n",
        "- Một vòng lặp for cho duyệt từ với chỉ số trang tới 600 và lặp lại 14 topic.\n",
        "\n",
        "  ```\n",
        "  base_url = \"https://www.thetimes.co.uk/topic\"\n",
        "\n",
        "  for cp in np.arange(1,600):\n",
        "      for j in range(14):\n",
        "        url = base_url + \"/\" + topic[j] + \"?page=\" + str(cp)\n",
        "  ```\n",
        "\n",
        "- Ở **The Sun Times** có một **Topic** mà nhóm tìm thấy và tận dụng giống với các trang tin châm biến, có số trang và mỗi trang là tĩnh *(không có Load More và Scroll)* nên chỉ cần sử dụng BeautifulSoup để tìm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlbVtAjyASR_"
      },
      "source": [
        "##**5. The Washington Post**\n",
        "\n",
        "- Trang này là tốn nhiều thời gian nhất của nhóm. Đây là trang web động, việc muốn xem thêm các bài báo sẽ phải click vào nut **Load More**. \n",
        "\n",
        "- nhóm đã sử dụng Selenium với click() để load thêm nhiều  bài báo\n",
        "  ```\n",
        "  browser.find_element_by_xpath('//*[@id=\"f0hXwRD2lz28ss\"]/div/div[1]/div[2]').click()\n",
        "  ```\n",
        "- Tuy nhiên, nhóm chỉ thư được những bài hiển thị mặc định chứ không có những bài tải thêm. Nhóm cũng đã thử bằng Scrapy lẫn BeautifulSoup + Selenium nhưng kết quả vẫn không thay đổi. Code cả hai nhóm em đều bỏ vào trong notebook chưa code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Sgp0XVDxux"
      },
      "source": [
        "##**6. The Dailyer**\n",
        "\n",
        "- Trang tim châm biếm này nhóm lựa chọn thay thế cho trang Cracker.\n",
        "\n",
        "- Nhìn chung, ở trang The Dailyer có cấu trúc giống với các trang tin châm biếm khác nên chỉ cần sử dụng BeautifulSoup là có thể crawl được.\n",
        "\n",
        "- Nhóm lấy các category của trang dưới đây:\n",
        "```\n",
        "category = [\"world\", \"feat\", \"local\", \"nat\", \"sport\", \"opinion\"]\n",
        "```\n",
        "\n",
        "\n",
        "- Tuy nhiên ở mỗi category có số lượng trang (page) khác nhau nên nhóm phải đặt điều kiện để tránh bị lỗi 404."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUgwujsAreJk"
      },
      "source": [
        "##**7. The Newsweek**\n",
        "\n",
        "- Đây là trang thay thế cho *The New York Times* mà nhóm không crawl dữ liệu được.\n",
        "- Do có kinh nghiệm và code của những trang trước nên trang này cũng làm tương tự. Một điều đáng lưu ý là **The Newsweek** chặn sử dụng *urllib* nên gây ra lỗi **HTTP Error 403: Forbidden**. Khắc phục bằng cách thêm dùng code dưới đây trước khi gửi yêu cầu tới trang.\n",
        "```\n",
        "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "```"
      ]
    }
  ]
}