{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-lx2PmxAC_Z"
      },
      "source": [
        "<center> <h1> <b> CS114.L21: MACHINE LEARNING <center> <h1> <b>\n",
        "\n",
        "<center> <h2> <b> BÀI TẬP COLAB SỐ 04: WEB SCRAPING (Ngày 07/6/2021) <center> <h> <b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ATgx6cmAEhy"
      },
      "source": [
        "###**1. Nhóm thực hiện**\n",
        "\n",
        "* 19522246 - Vũ Nguyễn Nhật Thanh\n",
        "* 19522180 - Trương Thế Tấn\n",
        "* 19521551 - Nông Thanh Hồng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4GTR29nWEmc"
      },
      "source": [
        "#**I. Trang châm biếm**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WAyRNyiZefY"
      },
      "source": [
        "###*Code dùng chung cho trang tin Châm biếm*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gG-NZWHZxr6"
      },
      "source": [
        "- **Thư viện chung:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls86JrYtZeAo"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as ticker\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNxnvkGvaU75"
      },
      "source": [
        "- **Lưu data thành file .csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7JGHDcHZn0H"
      },
      "source": [
        "df = pd.DataFrame({'is_sarcastic': 1,\n",
        "                   'title': title, \n",
        "                   'article_link': article_link,\n",
        "                   'publishdate': publishdate})\n",
        "\n",
        "#Code được viết trên colab nên dẫn link lưu lại qua drive\n",
        "df.to_csv(\"/content/gdrive/MyDrive/Colab Notebooks/Machine Learning/31062021_sarcasm-detection/dataset/<name>.csv\", header=True, index=False)\n",
        "df.to_csv(\"/content/<name>.csv\", header=True, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHGDmQLfXJSe"
      },
      "source": [
        "##**1. The Beaverton**\n",
        "\n",
        "- https://www.thebeaverton.com/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELxX944tYqEi"
      },
      "source": [
        "#https://www.datacamp.com/community/tutorials/tutorial-python-beautifulsoup-datacamp-tutorials\n",
        "title = []\n",
        "article_link = []\n",
        "publishdate = []\n",
        "\n",
        "for cp in np.arange(1,500):\n",
        "    url = \"https://thebeaverton.com/page/\" + str(cp)\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "    header = soup.findAll(class_ = \"post-title entry-header\")\n",
        "    datetime = soup.findAll(\"aside\", {\"class\": \"post-author cf\"})\n",
        "\n",
        "    for i in header:\n",
        "      article_link.append(i.a['href'])\n",
        "      title.append(i.a['title'])\n",
        "\n",
        "    for t in datetime:\n",
        "      publishdate.append(t.time['datetime'])\n",
        "    for t in range(111):\n",
        "      publishdate.append(\"NaN\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7z_z-mAXg5_"
      },
      "source": [
        "##**2. The Burrard Street Journal**\n",
        "\n",
        "- https://www.burrardstreetjournal.com/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2OE3LVHY_EN"
      },
      "source": [
        "title = []\n",
        "article_link = []\n",
        "publishdate = []\n",
        "\n",
        "for cp in np.arange(1,48):\n",
        "    url = \"https://www.burrardstreetjournal.com/page/\" + str(cp)\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "    header = soup.findAll(class_ = \"entry-title td-module-title\")\n",
        "    datetime = soup.findAll(\"span\", {\"class\": \"td-post-date\"})\n",
        "\n",
        "    for i in header:\n",
        "      article_link.append(i.a['href'])\n",
        "      title.append(i.a['title'])\n",
        "\n",
        "\n",
        "    for t in datetime:\n",
        "      publishdate.append(t.time['datetime'])\n",
        "\n",
        "    if (cp == 1):\n",
        "      for t in range(len(title) - len(publishdate)):\n",
        "        publishdate.append(\"NaN\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCx2k7vGXt-I"
      },
      "source": [
        "##**3. The Dailyer:**\n",
        "\n",
        "- http://thedailyer.com/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZTfhcH2l8se"
      },
      "source": [
        "#Link mẫu của The dailyer\n",
        "##http://thedailyer.com/category/local/page/2/\n",
        "\n",
        "title = []\n",
        "article_link = []\n",
        "\n",
        "# category: 6\n",
        "category = [\"world\", \"feat\", \"local\", \"nat\", \"sport\", \"opinion\"]\n",
        "for j in range(6):\n",
        "  print (category[j])\n",
        "  for cp in range(1,75):\n",
        "    print (cp)\n",
        "    url = \"http://thedailyer.com/category/\" + category[j] + \"/page/\" + str(cp)\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "    header = soup.findAll(class_ = \"title entry-title\")\n",
        "\n",
        "    for i in header:\n",
        "      article_link.append(i.a['href'])\n",
        "      title.append(i.text)\n",
        "\n",
        "    # do mỗi category có số trang khác nhau phải kiểm tra và thoát để không bị lỗi 404\n",
        "    if j == 0 and cp == 6:\n",
        "      break\n",
        "    elif j == 2 and cp == 60:\n",
        "      break\n",
        "    elif j == 3 and cp == 26:\n",
        "      break\n",
        "    elif j == 4 and cp == 20:\n",
        "      break\n",
        "    elif j == 5 and cp == 16:\n",
        "      break\n",
        "      \n",
        "#in ra số phần tử của mỗi list article_link, title, puplishdate\n",
        "print (len(article_link), len(title))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50eQyV4MYAgU"
      },
      "source": [
        "#**II. Trang báo chính thống**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlWoVJ5VaHrm"
      },
      "source": [
        "###*Code dùng chung cho trang báo Chính thống*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0EEMM83amdM"
      },
      "source": [
        "- **Cài đặt các gói cần thiết để chạy được trên Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f22pNghmaH_Z"
      },
      "source": [
        "# install chromium, its driver, and selenium\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "# set options to be headless, ..\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-eU0-ma2Vp"
      },
      "source": [
        "- **Thư viện chung**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo8NZKtqavaN"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPCDs3MpbkxT"
      },
      "source": [
        "- **Lưu file csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2EeMEkKbX14"
      },
      "source": [
        "df = pd.DataFrame({'is_sarcastic': 0,\n",
        "                   'title': title, \n",
        "                   'article_link': article_link,\n",
        "                   'publishdate': publishdate})\n",
        "\n",
        "df.to_csv(\"/content/gdrive/MyDrive/Colab Notebooks/Machine Learning/31062021_sarcasm-detection/dataset/<name>.csv\", header=True, index=False)\n",
        "df.to_csv(\"/content/<name>.csv\", header=True, index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVEUr5MQYGg-"
      },
      "source": [
        "##**1. New York Times**\n",
        "\n",
        "- https://www.nytimes.com/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAIeEFE0a9Fv"
      },
      "source": [
        "# Example url: https://www.nytimes.com/issue/todayspaper/2020/06/03/todays-new-york-times\n",
        "\n",
        "year = [\"2018\", \"2019\", \"2020\", \"2021\"]\n",
        "month = [\"12\", \"11\", \"10\", \"09\", \"08\", \"07\", \"06\", \"05\", \"04\", \"03\", \"02\",\"01\"]\n",
        "date = [\"01\", \"02\", \"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\"]\n",
        "\n",
        "base_url = \"https://www.nytimes.com/issue/todayspaper/\"\n",
        "\n",
        "\n",
        "article_link = []\n",
        "title = []\n",
        "publishdate = []\n",
        "\n",
        "for y in year:\n",
        "    if (y == \"2021\"):\n",
        "        month.clear()\n",
        "        month = [\"05\", \"04\", \"03\", \"02\",\"01\"]\n",
        "    for m in month:\n",
        "        for d in date:\n",
        "            urls = base_url + y + \"/\" + m + \"/\" + d + \"/todays-new-york-times\"\n",
        "\n",
        "            browser = webdriver.Chrome('chromedriver',options=options)\n",
        "\n",
        "            browser.get(urls)\n",
        "\n",
        "            #scroll page\n",
        "            scroll_pause_time = 5\n",
        "            i = 0\n",
        "            while i < 4:\n",
        "                if (i != 0):\n",
        "                    browser.execute_script(\"window.scrollTo(0, 0);\")\n",
        "                    time.sleep(2)\n",
        "                    browser.execute_script(\n",
        "                        \"window.scrollTo(0, ((document.body.scrollHeight*9)/10));\")\n",
        "                    # Wait to load page\n",
        "                    time.sleep(scroll_pause_time)\n",
        "                else:\n",
        "                    browser.execute_script(\n",
        "                        \"window.scrollTo(0, (document.body.scrollHeight*3)/4);\")\n",
        "                i += 1\n",
        "\n",
        "            \n",
        "            soup = BeautifulSoup(browser.page_source, 'html')\n",
        "            browser.close()\n",
        "            detail = soup.findAll('li', attrs={'class':\"css-i435f0\" })\n",
        "\n",
        "            for cp in detail:\n",
        "                title_vs_url = cp.find('div', attrs={'class':\"css-141drxa\"})\n",
        "            \n",
        "                article_link.append(\"https://www.nytimes.com/\" + title_vs_url.a['href'])\n",
        "\n",
        "                temp_title = title_vs_url.find('h2', attrs={\"class\":\"css-ds6ff4 ehnp9uj0\"})\n",
        "                title.append(temp_title.find(text = True))\n",
        "\n",
        "                publishdate.append(y + \"-\" + m + \"-\" + d)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDenAKOYGnV"
      },
      "source": [
        "##**2. The Sunday Times**\n",
        "\n",
        "- https://www.thetimes.co.uk/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxlqJl83bN1U"
      },
      "source": [
        "title = []\n",
        "article_link = []\n",
        "publishdate = []\n",
        "\n",
        "base_url = \"https://www.thetimes.co.uk/topic\"\n",
        "\n",
        "#14 topic\n",
        "topic = [ \"uk-politics\", \"global-politics\", \n",
        "          \"health\", \"technology\", \"transport\", \"science\", \"law\", \n",
        "          \"economics\", \"markets\", \"personal-finance\", \"real-estate\", \"banking\", \n",
        "          \"tennis\", \"golf\" ]\n",
        "\n",
        "\n",
        "for cp in np.arange(1,600):\n",
        "    for j in range(14):\n",
        "      url = base_url + \"/\" + topic[j] + \"?page=\" + str(cp)\n",
        "\n",
        "      page = requests.get(url)\n",
        "      soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "      header = soup.findAll(\"div\", attrs = {\"class\": \"responsiveweb__ListContentContainer-sc-143eup1-0 isZXs responsiveweb__ListContentContainer-sc-143eup1-0 isZXs css-1dbjc4n\"})\n",
        "      text_title = soup.findAll(\"h3\", attrs = {\"class\": \"css-4rbku5 css-901oao r-1khnkhu r-iirzy8 r-evnaw r-16dba41 r-1kt6imw r-d0pm55\"})\n",
        "      date_time = soup.findAll(\"div\", attrs={\"class\": \"css-901oao r-1khp51w r-j2s0nr r-n6v787 r-fxxt2n r-d0pm55\"})\n",
        "\n",
        "      for temp in header:\n",
        "        if (len(temp.a['href']) > 10):\n",
        "          article_link.append(temp.a['href'])\n",
        "\n",
        "      for t in text_title:\n",
        "        title.append(t.text)\n",
        "\n",
        "      for x in date_time:\n",
        "        publishdate.append(x.time['datetime'])\n",
        "       \n",
        "\n",
        "print (len(title), len(article_link), len(publishdate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ0Zy6ucYXuc"
      },
      "source": [
        "##**3. The Washington Post**\n",
        "\n",
        "- https://www.washingtonpost.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqe4QunWAfWL"
      },
      "source": [
        "- **Code Scrapy + Selenium:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ej9sP3dVQC9"
      },
      "source": [
        "\n",
        "import scrapy\n",
        "from selenium import webdriver\n",
        "\n",
        "\n",
        "class WashingtonPostspider(scrapy.Spider):\n",
        "    name = 'washingtonpost'\n",
        "    allowed_domains = ['washingtonpost.com']\n",
        "    start_urls = ['https://www.washingtonpost.com/politics/?itid=nb_politics']\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        self.driver = webdriver.Chrome(\n",
        "            \"C:/Users/Admin/washingtonpost/washingtonpost/chromedriver.exe\")\n",
        "\n",
        "    def parse(self, response):\n",
        "\n",
        "        self.driver.get(response.url)\n",
        "\n",
        "        for i in range(600):\n",
        "            load_more = self.driver.find_element_by_xpath('//*[@id=\"f0hXwRD2lz28ss\"]/div/div[1]/div[2]').click()\n",
        "\n",
        "        \n",
        "        titles = response.xpath(\n",
        "                            \"//div[@class = 'story-headline']/h2/a/text()\").extract()\n",
        "\n",
        "        article_links = response.xpath(\n",
        "                            \"//div[@class = 'story-headline']/h2/a/@href\").extract()\n",
        "\n",
        "        row_data = zip(titles,article_links)\n",
        "\n",
        "        for item in row_data:\n",
        "            scraped_info = {\n",
        "                            'title': item[0],\n",
        "                            'article_link': item[1]\n",
        "            }\n",
        "            yield scraped_info\n",
        "\n",
        "        self.driver.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gBNh9nPAlhJ"
      },
      "source": [
        "- **Code BeautifulSoup + Selenium:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwzJmaAwAdsJ"
      },
      "source": [
        "#launch url\n",
        "url_Politics = \"https://www.washingtonpost.com/politics/?itid=nb_front_politics\"\n",
        "\n",
        "browser = webdriver.Chrome('chromedriver',options=options)\n",
        "\n",
        "article_link = []\n",
        "title = []\n",
        "\n",
        "for i in range(10):\n",
        "  browser.get(url_Politics)\n",
        "  browser.find_element_by_xpath('//*[@id=\"f0hXwRD2lz28ss\"]/div/div[1]/div[2]').click()\n",
        "\n",
        "time.sleep(5) \n",
        "\n",
        "# Create BeautifulSoup object from page source.\n",
        "soup = BeautifulSoup(browser.page_source, 'html')\n",
        "\n",
        "      \n",
        "# Parse and extract the data that you need.\n",
        "detail = soup.findAll(\"div\", {'class': 'story-headline'})\n",
        "\n",
        "for cp in detail:\n",
        "  temp = cp.find('h2', attrs={'class':\"\"})\n",
        " \n",
        "  if (temp != None):\n",
        "    article_link.append(temp.a['href'])\n",
        "    title.append(temp.find(text = True))\n",
        "\n",
        "print (len(title), len(article_link))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DNmQA5Ym2_J"
      },
      "source": [
        "##**4. Newsweek**\n",
        "\n",
        "- https://www.newsweek.com/\n",
        "\n",
        "- Nhóm chọn trang này để thay thế cho Trang *The Washington Post* do không crawl được dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7xWRhgwnTkv"
      },
      "source": [
        "#link mẫu:\n",
        "# https://www.newsweek.com/world?page=9\n",
        "\n",
        "title = []\n",
        "article_link = []\n",
        "\n",
        "category = ['world', \"business\", \"tech-science\", \"culture\", \"newsgeek\", \"sports\", \"health\", \"opinion\", \"education\"]\n",
        "\n",
        "for j in category:\n",
        "  print (j)\n",
        "  for cp in np.arange(1, 5):\n",
        "    url = \"https://www.newsweek.com/\" + category[j] + \"?page=\" + str(cp)\n",
        "\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(url,headers=hdr)\n",
        "    page = urlopen(req)\n",
        "    soup = BeautifulSoup(page)\n",
        "\n",
        "    header = soup.findAll('div', attrs={\"class\": \"inner\"})\n",
        "\n",
        "    for i in header:\n",
        "      t = i.find('h3')\n",
        "      article_link.append(\"https://www.newsweek.com\" + t.a['href'])\n",
        "      title.append(t.text)\n",
        "\n",
        "    print (cp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y5VuVPX7Zyg"
      },
      "source": [
        "#**III. Code nối các file .csv và chuyển sang json**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NfKD9ao8w-_"
      },
      "source": [
        "- **Nối file csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Uqfth-b7nxr"
      },
      "source": [
        "satiric_1 = pd.read_csv(\"/content/1_TheBurrardStreetJournal.csv\")\n",
        "satiric_2 = pd.read_csv(\"/content/1_TheBeaverton.csv\")\n",
        "satiric_3 = pd.read_csv(\"/content/1_TheDailyEr.csv\")\n",
        "satiric = satiric_1.append(satiric_2)\n",
        "satiric.append(satiric_3)\n",
        "\n",
        "official_1 = pd.read_csv(\"/content/0_TheSunTime.csv\")\n",
        "official_2 = pd.read_csv(\"/content/0_TheNewYorkTime.csv\")\n",
        "official_3 = pd.read_csv(\"/content/0_TheNewYorkTime.csv\")\n",
        "official = official_2.append(official_1)\n",
        "official.append(official_3)\n",
        "\n",
        "satiric.to_csv(\"Sarcasm_Detection_v1.csv\",index=False)\n",
        "official.to_csv(\"Sarcasm_Detection_v2.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnRGKa1z81WS"
      },
      "source": [
        "* **Chuyển csv sáng json**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SiHo0D_85Og"
      },
      "source": [
        "def csv_to_json(csvFilePath, jsonFilePath):\n",
        "    jsonArray = []\n",
        "      \n",
        "    #read csv file\n",
        "    with open(csvFilePath, encoding='utf-8') as csvf: \n",
        "        #load csv file data using csv library's dictionary reader\n",
        "        csvReader = csv.DictReader(csvf) \n",
        "\n",
        "        #convert each csv row into python dict\n",
        "        for row in csvReader: \n",
        "            #add this python dict to json array\n",
        "            jsonArray.append(row)\n",
        "  \n",
        "    #convert python jsonArray to JSON String and write to file\n",
        "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
        "        jsonString = json.dumps(jsonArray, indent=4)\n",
        "        jsonf.write(jsonString)\n",
        "          \n",
        "csvFilePath = r\"/content/Sarcasm_Detection_v1.csv\"\n",
        "jsonFilePath = r\"/content/Sarcasm_Detection_v1.json\"\n",
        "csv_to_json(csvFilePath, jsonFilePath)\n",
        "\n",
        "csvFilePath = r\"/content/Sarcasm_Detection_v2.csv\"\n",
        "jsonFilePath = r\"/content/Sarcasm_Detection_v2.json\"\n",
        "csv_to_json(csvFilePath, jsonFilePath)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}